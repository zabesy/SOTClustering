{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 131923,
     "status": "ok",
     "timestamp": 1633372462460,
     "user": {
      "displayName": "adam-nord@hotmail.com",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12454353131717006596"
     },
     "user_tz": -120
    },
    "id": "WikwhI2T2ozI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Short display of the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": "              school         author                                  title  \\\n529  german_idealism  Immanuel Kant                critique of pure reason   \n416        aristotle      Aristotle  the complete works of aristotle vol 1   \n\n                                    tokenized_sentence  num_of_chars  \\\n529  [concepts, four, paralogisms, transcendental, ...           111   \n416  [mutilated, creatures, drag, wounded, limb, re...            70   \n\n     num_of_words                                    string_sentence  \n529            14  concepts four paralogisms transcendental doctr...  \n416             9  mutilated creatures drag wounded limb remainde...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>school</th>\n      <th>author</th>\n      <th>title</th>\n      <th>tokenized_sentence</th>\n      <th>num_of_chars</th>\n      <th>num_of_words</th>\n      <th>string_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>529</th>\n      <td>german_idealism</td>\n      <td>Immanuel Kant</td>\n      <td>critique of pure reason</td>\n      <td>[concepts, four, paralogisms, transcendental, ...</td>\n      <td>111</td>\n      <td>14</td>\n      <td>concepts four paralogisms transcendental doctr...</td>\n    </tr>\n    <tr>\n      <th>416</th>\n      <td>aristotle</td>\n      <td>Aristotle</td>\n      <td>the complete works of aristotle vol 1</td>\n      <td>[mutilated, creatures, drag, wounded, limb, re...</td>\n      <td>70</td>\n      <td>9</td>\n      <td>mutilated creatures drag wounded limb remainde...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_csv(\"philosophy_data.csv\", converters={\"tokenized_sentence\": pd.eval})\n",
    "\n",
    "#Smaller dataset\n",
    "# DF = DF.sample(1000)\n",
    "# DF.reset_index(drop = True, inplace = True)\n",
    "\n",
    "print(len(DF))\n",
    "DF.sample(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract the tokenized_sentence DF[\"tokenized_sentence\"] sentences and append them into a python list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# sentences = []\n",
    "# sentences = collect_sentences_from_tokenized(sentences, DF[\"tokenized_sentence\"])\n",
    "# sentences[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load w2v model, tf_idf, bigram, and trigram models and save them in respective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "bigram_range = (2,2)\n",
    "trigram_range = (3,3)\n",
    "#Remove those terms that occur less than % in the documents count\n",
    "min_df = 0.001\n",
    "#Remove those terms that occur more than % in the documents count\n",
    "max_df = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((1000, 4702), (1000, 10161), (1000, 9376), (1000, 10161), (1000, 9376))"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w2v = load_and_or_save_w2v_model(file_name=\"glove.42B.300d.txt.word2vec\", w2v_type=\"glove\", input_file=\"glove.42B.300d.txt\")\n",
    "tf_idf_vec = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", stopwords=None, min_df=min_df, max_df=max_df)\n",
    "tf_idf = tf_idf_vec.fit_transform(DF[\"string_sentence\"].tolist())\n",
    "# 1-2-gram\n",
    "bigram_vec = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", stopwords=None, min_df=min_df, max_df=max_df, ngram_range=bigram_range)\n",
    "bigram = bigram_vec.fit_transform(DF[\"string_sentence\"].tolist())\n",
    "tf_idf_bigram_vec = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",stopwords=None, min_df=min_df, max_df=max_df, ngram_range=bigram_range)\n",
    "tf_idf_bigram = tf_idf_bigram_vec.fit_transform(DF[\"string_sentence\"].tolist())\n",
    "# 1-3-gram\n",
    "trigram_vec = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",stopwords=None, min_df=min_df, max_df=max_df, ngram_range=trigram_range)\n",
    "trigram = trigram_vec.fit_transform(DF[\"string_sentence\"].tolist())\n",
    "tf_idf_trigram_vec = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",stopwords=None, min_df=min_df, max_df=max_df, ngram_range=trigram_range)\n",
    "tf_idf_trigram = tf_idf_trigram_vec.fit_transform(DF[\"string_sentence\"].tolist())\n",
    "\n",
    "tf_idf.shape, tf_idf_bigram.shape, tf_idf_trigram.shape, bigram.shape, trigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tf_idf_gen.get_feature_names_out()\n",
    "\n",
    "# print(type(tf_idf[0]))\n",
    "# print(tf_idf.getrow(0))\n",
    "# [x for x in tf_idf[:].toarray()[0] if x < 1e-8]\n",
    "\n",
    "# # To TEST MIN_DIF/MAX_DF\n",
    "# texts = [\n",
    "#     \"Penny bought bright blue fishes.\",\n",
    "#     \"Penny bought bright blue and orange fish.\",\n",
    "#     \"The cat ate a fish at the store.\",\n",
    "#     \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n",
    "#     \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "#     \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "#     \"Penny is a fish\"\n",
    "# ]\n",
    "# vec = TfidfVectorizer(min_df=1, max_df=1.0, stop_words='english')\n",
    "# ble = vec.fit_transform(texts)\n",
    "# pd.DataFrame(ble.toarray(), columns=vec.get_feature_names())\n",
    "# print(ble.shape)\n",
    "# ble_kmeans, ble_labels, ble_centroids = apply_kmeans(2, ble, tolerance=1e-8)\n",
    "# results = pd.DataFrame()\n",
    "# results['sentence'] = texts\n",
    "# results['category'] = ble_labels\n",
    "# results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dimension reduction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tf_idf = TruncatedSVD(n_components=100).fit_transform(tf_idf)\n",
    "# tf_idf_bigram = TruncatedSVD(n_components=100).fit_transform(tf_idf_bigram)\n",
    "# tf_idf_trigram = TruncatedSVD(n_components=100).fit_transform(tf_idf_trigram)\n",
    "# bigram = TruncatedSVD(n_components=100).fit_transform(bigram)\n",
    "# trigram = TruncatedSVD(n_components=100).fit_transform(trigram)\n",
    "#\n",
    "# tf_idf.shape, tf_idf_bigram.shape, tf_idf_trigram.shape, bigram.shape, trigram.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence_vectorizer function summarizes each vectorized word in a sentence and divides the sum with the amount of words in the sentence.\n",
    "Doing this will allow us to be able to get whole sentences vectorized values instead of individual words.\n",
    "Using the function with our model (in this case w2v) sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# indexes_to_remove_from_df = []\n",
    "# sentences_vectorized = []\n",
    "# for index, sentence in enumerate(sentences):\n",
    "#     temp_variable = sentence_vectorizer(sentence, w2v)\n",
    "#     #Check if return variable is empty\n",
    "#     if temp_variable is None:\n",
    "#         indexes_to_remove_from_df.append(index)\n",
    "#         continue\n",
    "#     sentences_vectorized.append(temp_variable)\n",
    "#     if index % 50000 == 0:\n",
    "#         print(index)\n",
    "#\n",
    "# #Remove rows (found in list) from DF, and reset indexing.\n",
    "# DF.drop(DF.index[indexes_to_remove_from_df], inplace=True)\n",
    "# DF.reset_index(drop = True, inplace = True)\n",
    "# sentences = collect_sentences_from_tokenized([], DF[\"tokenized_sentence\"])\n",
    "#\n",
    "# #Should be equal\n",
    "# len(sentences), len(DF[\"tokenized_sentence\"]), len(sentences_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fetch the number of unique School of Thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_schools = DF[\"school\"].unique().tolist()\n",
    "print(unique_schools)\n",
    "num_clusters = len(unique_schools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Bar plot how many sentences are from each School of Thought.\n",
    "(Show the total amount of real assigned SOT on each text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_dict = dict.fromkeys(unique_schools, 0)\n",
    "for school in DF[\"school\"]:\n",
    "    total_dict[school] += 1\n",
    "\n",
    "plt.bar(range(len(total_dict)), total_dict.values(), align='center')\n",
    "plt.xticks(range(len(total_dict)), total_dict.keys())\n",
    "plt.xticks(rotation=50)\n",
    "plt.xlabel(\"School of Thought\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Original Distribution of Sentences per School of Thought\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Apply kmeans with num_clusters=amount of unique schools in the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# kmeans, labels, centroids = apply_kmeans(num_clusters, sentences_vectorized)\n",
    "tf_idf_kmeans, tf_idf_labels, tf_idf_centroids = apply_kmeans(num_clusters, tf_idf, tolerance=1e-8)\n",
    "tf_idf_bigram_kmeans, tf_idf_bigram_labels, tf_idf_bigram_centroids = apply_kmeans(num_clusters, tf_idf_bigram,tolerance=1e-8)\n",
    "tf_idf_trigram_kmeans, tf_idf_trigram_labels, tf_idf_trigram_centroids = apply_kmeans(num_clusters, tf_idf_trigram,tolerance=1e-8)\n",
    "bigram_kmeans, bigram_labels, bigram_centroids = apply_kmeans(num_clusters, bigram,tolerance=1e-8)\n",
    "trigram_kmeans, trigram_labels, trigram_centroids = apply_kmeans(num_clusters, trigram,tolerance=1e-8)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Find top words in each cluster on tf_idf\n",
    "order_centroids = tf_idf_centroids.argsort()[:, ::-1]\n",
    "terms = tf_idf_vec.get_feature_names()\n",
    "for i in range(10):\n",
    "    top_ten_words = [terms[ind] for ind in order_centroids[i, :5]]\n",
    "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Summarize the amount each cluster was assigned to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# label_dict = dict.fromkeys(list(set(labels)), 0)\n",
    "# for cluster in labels:\n",
    "#     label_dict[cluster] += 1\n",
    "#\n",
    "# plt.bar(range(len(label_dict)), label_dict.values(), align='center')\n",
    "# plt.xticks(range(len(label_dict)), label_dict.keys())\n",
    "# plt.xlabel(\"Cluster\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.title(f\"Clustered sentence\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Apply scatter plot on kmeans results using TSNE.\n",
    "Scatter plot using TSNE, PCA and UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# scatter_plot(type='tsne', labels=labels, values=sentences_vectorized, centroids=centroids)\n",
    "# scatter_plot(type='pca', labels=labels, values=sentences_vectorized, centroids=centroids)\n",
    "# scatter_plot(type='umap', labels=labels, values=sentences_vectorized, centroids=centroids)\n",
    "\n",
    "# tf_idf_reduced = TruncatedSVD(500).fit_transform(tf_idf)\n",
    "# tf_idf_bigram_reduced = TruncatedSVD(500).fit_transform(tf_idf_bigram)\n",
    "# tf_idf_trigram_reduced = TruncatedSVD(500).fit_transform(tf_idf_trigram)\n",
    "#\n",
    "# #PCA\n",
    "# scatter_plot(type='pca', labels=tf_idf_labels, values=tf_idf_reduced, centroids=tf_idf_centroids)\n",
    "# scatter_plot(type='pca', labels=tf_idf_bigram_labels, values=tf_idf_bigram_reduced, centroids=tf_idf_bigram_centroids)\n",
    "# scatter_plot(type='pca', labels=tf_idf_trigram_labels, values=tf_idf_trigram_reduced, centroids=tf_idf_trigram_centroids)\n",
    "#\n",
    "# #UMAP\n",
    "# scatter_plot(type='umap', labels=tf_idf_labels, values=tf_idf_reduced, centroids=tf_idf_centroids)\n",
    "# scatter_plot(type='umap', labels=tf_idf_bigram_labels, values=tf_idf_bigram_reduced, centroids=tf_idf_bigram_centroids)\n",
    "# scatter_plot(type='umap', labels=tf_idf_trigram_labels, values=tf_idf_trigram_reduced, centroids=tf_idf_trigram_centroids)\n",
    "#\n",
    "# #TSNE\n",
    "# scatter_plot(labels=tf_idf_labels, values=tf_idf_reduced, centroids=tf_idf_centroids)\n",
    "# scatter_plot(labels=tf_idf_bigram_labels, values=tf_idf_bigram_reduced, centroids=tf_idf_bigram_centroids)\n",
    "# scatter_plot(labels=tf_idf_trigram_labels, values=tf_idf_trigram_reduced, centroids=tf_idf_trigram_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Combine the kmeans clustering results with the labels.\n",
    "Put the kmeans result in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#How many times cluster n was assigned to specific SOT\n",
    "# cluster_n_assigned(DF=DF, labels=labels, schools=unique_schools, name='glove')\n",
    "cluster_n_assigned(DF=DF, labels=tf_idf_labels, schools=unique_schools, name='tf_idf')\n",
    "cluster_n_assigned(DF=DF, labels=tf_idf_bigram_labels, schools=unique_schools, name='tf_idf_bigram')\n",
    "cluster_n_assigned(DF=DF, labels=tf_idf_trigram_labels, schools=unique_schools, name='tf_idf_trigram')\n",
    "cluster_n_assigned(DF=DF, labels=bigram_labels, schools=unique_schools, name='bigram')\n",
    "cluster_n_assigned(DF=DF, labels=trigram_labels, schools=unique_schools, name='trigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Make a list and convert all \"plato\" to 0 and \"aristotle\" to 1, etc.\n",
    "This can then be applied with the predicted labels with v_measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_labels = [unique_schools.index(school) for school in DF[\"school\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Apply v-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# v_measure = v_measure_score(original_labels, labels)\n",
    "tf_idf_v_measure = v_measure_score(original_labels, tf_idf_labels)\n",
    "tf_idf_bigram_v_measure = v_measure_score(original_labels, tf_idf_bigram_labels)\n",
    "tf_idf_trigram_v_measure = v_measure_score(original_labels, tf_idf_trigram_labels)\n",
    "bigram_v_measure = v_measure_score(original_labels, bigram_labels)\n",
    "trigram_v_measure = v_measure_score(original_labels, bigram_labels)\n",
    "\n",
    "tf_idf_v_measure, tf_idf_bigram_v_measure, tf_idf_trigram_v_measure, bigram_v_measure, trigram_v_measure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the best v_measure for all different text vectorization methods.\n",
    "Apply different values for min_df and max_df for them and store it in a dict.\n",
    "Iterations are decided by min_df range * max_df range."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v_measure_dict, exceptions_dict = average_v_measure(orig_labels=original_labels, num_clusters=num_clusters,DF=DF, max_df_range=(0.005,0.1,0.005), min_df_range=(0.00001,0.0002,0.00001),n=20*20, bigram_range=bigram_range,trigram_range=trigram_range)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print v_measure values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_v_measure_dict = {}\n",
    "for method, list_of_dicts in v_measure_dict.items():\n",
    "    sorted_v_measure_dict[method] = sorted(list_of_dicts, key=lambda x: x[\"v_measure\"], reverse=True)\n",
    "\n",
    "#Prints the best v_measure results of each method.\n",
    "sorted_v_measure_dict[\"tf_idf\"][0], sorted_v_measure_dict[\"tf_idf_bigram\"][0], sorted_v_measure_dict[\"tf_idf_trigram\"][0], sorted_v_measure_dict[\"bigram\"][0], sorted_v_measure_dict[\"trigram\"][0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPqtR0V7cu/9c8zZfHqStSr",
   "collapsed_sections": [],
   "mount_file_id": "1Y_Pi1BIYybISVb00nrSP3J2ZVOMl24bU",
   "name": "ThesisClustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}